---
title: "Group 1: Multivariate analysis of australian climate data"
author:  "Andrea Iglesias Munilla, Kathryn Weissman, Diana Galindo Gonz치lez, Mateo J치come Gonz치lez y Pedro Gonz치lez Prado"
date: "`r Sys.Date()`"
output:
  html_notebook:
   toc: true
   toc_depth: 2
   toc_float: true
   theme: cerulean
   highlight: tango
   #collapsed: false
   #smooth_scroll: false
   number_sections: true
   
#theme: readable
#highlight: success #https://bootswatch.com/3/
#subtitle: Report
---
```{r setup, include=FALSE, results='hide'}
rm(list=ls(all=TRUE))

#INSTALL & LOAD LIBRARIES
# Required packages
pkgs<-c("rstudioapi","tidyverse","DT","naniar","tidyr","sf","ggplot","ggplot2","cowplot", "googleway", "ggplot2", "ggrepel", "VIM","ggspatial", "libwgeom", "sf", "rnaturalearth", "rnaturalearthdata","gridExtra","grid","ggplot2","lattice","FactoMineR","factoextra","corplot","heatmaply")

# Non-installed packages
inspkgs<-pkgs[!pkgs %in% installed.packages()]
for(libs in inspkgs) install.packages(libs)

# Loading required
sapply(pkgs,require,character=TRUE)

#("ade4","corrplot","factoextra","FactoMineR","foreign","ggplot2","gridExtra","Hmisc","RColorBrewer","reshape2","RPostgreSQL","knitr","openxlsx","NbClust","DT","d3heatmap","heatmaply","sf","viridis","leaflet","pander","VIM","plotly")
```
```{r}

# CLEARING ENVIRONMENT #

rm(list=ls(all=TRUE))

# LIBRARIES 

library(dplyr)
library(rstudioapi)

#PREPARE WORK DIRECTORY AND IMPORT DATASET

current_path <- getActiveDocumentContext()$path 
setwd(dirname(current_path ))

df <- read.csv("weatherAUSOriginal.csv", stringsAsFactors = T)
```
# Problem statement and selected dataset 

<<<<<<< HEAD
This project seeks to develop, train and evaluate a statistical model that helps end-users make predictions about whether or not there will be rainfall the following day, given the weather conditions on a given day in Australia. In addition, several multivariate techniques will be implemented in order to extract key insights and relevant information from the available historical data. A better understanding of the factors influencing rainfall in Australia and how these may have changed over time given the harsh climate changes the territory has experienced in the last decade may be of use to predict future droughts and wildfire crises. 

The main data set was obtained from the repository at [Kaggle.com][1]. This data set is built using publicly available climate data, provided by the [Australian Bureau of Meteorology][2] and measured by weather stations distributed across the country. It contains more than 100,000 daily weather observations over a 10-year period from 2007 to 2017 from 49 unique locations in Australia. These observations include temperature, rainfall, atmospheric pressure, evaporation, humidity, wind direction, and wind speed at different times during the day.  More specifically, the dataset contains 24 columns in total. Columns 1 through 21 are defined by the [Australian Government Bureau of Meteorology][2]: 

1. **Date**: date of the observation. 
2. **Location**: common name for the weather station location. 
3. **MinTemp**: minimum temperature in the 24 hours to 9am, degrees Celsius.
4. **MaxTemp**: maximum temperature in the 24 hours from 9am, degrees Celsius.
5. **Rainfall**: rainfall in the 24 hours to 9am, millimeters. 
6. **Evaporation**: "Class A" pan evaporation in the 24 hours to 9am, millimeters.
7. **Sunshine**: number of hours of bright sunshine in the 24 hours to midnight.
8. **WindGustDir**: direction of the strongest wind gust in the 24 hours to midnight, 
measured with 16 compass points.  
9. **WindGustSpeed**: speed of strongest wind gust in the 24 hours to midnight, km/h.
10. **WindDir9am**: wind direction  at 9am, measured with 16 compass points. 
11. **WindDir3pm**: wind direction at 3pm, measured with 16 compass points. 
12. **WindSpeed9am**: average wind speed over the 10-minute period prior to 9am, km/h.
13. **WindSpeed3pm**: average wind speed over the 10-minute period prior to 3pm, km/h.
14. **Humidity9am**: relative humidity percentage at 9am. 
15. **Humidity3pm**: relative humidity percentage at 3pm.
16. **Pressure9am**:  atmospheric pressure (hpa) reduced to mean sea level at 9am.
17. **Pressure3pm**: atmospheric pressure (hpa) reduced to mean sea level at 3pm.
18. **Cloud9am**: fraction of sky obscured by cloud at 9am, measured in oktas, a unit of eights that describes the amount of cloud cover at any given location such as a weather station, ranging from 0 (completely clear sky) to 8 (completely covered sky). Value 9 (sky obstructed from view) https://en.wikipedia.org/wiki/Okta 
19. **Cloud3pm**: fraction of sky obscured by cloud at 3pm, measured in oktas.
20. **Temp9am**: temperature at 9am, measured in degree celsius. 
21. **Temp3pm**: temperature at 3pm, measured in degree celsius.
22. **RainToday**: boolean variable; Yes if precipitation (mm) in the 24 hours to 9am exceeds 1 mm, otherwise No. 
23. **RainTomorrow**: boolean variable; Yes if the following day precipitation exceeds 1 mm, otherwise No. 
=======
```{r, results='hide'}```
>>>>>>> a74839c4f70fead1adb9073e3948a2f5cac31335

## Checking variables

```{r}
#LIST VARIABLES
str(df)
```

## Updating Variables

For convenience, the project team adds an ID variable and defines the levels
of wind direction based on clockwise orientation of the compass points. The
Date variable is also formatted as a date. In order to use some of the data 
analysis algorithms, the date will be better interpreted as Year, Month, 
and Day of month, so columns are added for these variables.

```{r}
#CHANGE VARIABLE TYPE
df$Date <- as.Date(df$Date)

###TO DO: Add ID column

###TO DO: SET LEVELS FOR WIND DIRECTION

#df$WindGustDir <- 
#df$WindDir9am <- 
#df$WindDir3pm <- 

df[, "Year"] <- as.factor(format(df[,"Date"], "%Y"))
df[, "Month"] <- as.factor(format(df[,"Date"], "%B"))
df[, "Day"] <- as.integer(format(df[,"Date"], "%d"))
df <- df %>% relocate(c("Year","Month","Day"), .after = "Date")

```

## Splitting Data

A different project team in Group 12 chose the same data set, so this team will
work with the first half of the data set, divided by date, as instructed by
the lab professor on 27-Sep-2021.

We will use the median date to split the data set and keep the first half.

```{r}
#SUBSET DATA BY MEDIAN DATE
df <- subset(df, Date < median(df$Date))
```

## Summary Statistics

```{r}
#SUMMARIZE DATA BY COLUMN
summary(df)
```



## Detecting Missing Values

```{r}
#Display a table that shows the number of rows with a certain number of NA's
mis_ind = rowSums(is.na(df))
table(mis_ind)
```

```{r}
#Plot the distribution of the number of NA's per row
hist(mis_ind)
```

In order to build the model, observations should be removed that are missing
too much data. Observations that have NA values above the 90th percentile of
the NA count distribution should be removed.

```{r}
#Check 90th percentile of missing data and save value to variable
rm_NA <- quantile(mis_ind,0.90)
```

We should remove observations with more than 6 NA values.

```{r}
#Creates an index and data frame of observations that have missing data
#above the cut-off threshold.
m1 <- which(mis_ind > rm_NA)
df_remove1 <- df[m1,]

#Removes observations with too many NA's from data frame used for modeling
df <- df[-c(m1),]
```


```{r}
#Summarizes the number of NA's per variable.
mis_col = colSums(is.na(df))
mis_col
```

```{r}
#Check number of unique locations and summary
num_unique_locations <- length(unique(df$Location))
summary(df$Location)
plot(df$Location)

#Check number of observations below the 10% quantile 
rm_Loc <- quantile(summary(df$Location),0.1)
```

Locations with less than 1101 observations will be removed since they are
below the 10th percentile of number of recorded observations.

```{r}
#Creates an index and data frame of Locations that have missing data
#below the cut-off threshold.
m2 <- which(summary(df$Location) < rm_Loc)
loc_remove <- levels(df$Location)[c(m2)]
df_remove2 <- subset(df,Location %in% c(loc_remove))

#REMOVE OBSERVATIONS FROM SPECIFIC LOCATIONS
df <- subset(df,!Location %in% c(loc_remove))
```

5 locations were removed from the data set because of too few observations.
They were Katherine, MountGinini, Newcastle, Nhil, and Uluru.

```{r}
#CHECK NUMBER OF UNIQUE DATES AND TIME SPAN
num_unique_dates <- length(unique(df$Date))
time_difference <- as.numeric((max(df$Date)-min(df$Date)), units="days")
total_years <- time_difference/365
```

There are weather observations for 1951 unique dates across 44 unique locations 
over the time span of approximately 5.5 years.

```{r}
#CALCULATE NUMBER OF MISSING DATES
num_missing_dates <- time_difference - num_unique_dates
```

There are 1951 unique dates in the data set, however there is a difference of
2039 days between the first observation and the last observation, which means
there are 88 dates missing in the time span.


## Choosing Locations to Model

Due to the geographic dependencies of the rainfall model, and the amount of data
included in the data set, we will reduce the amount of data by keeping only
specific locations. We would like to model locations from different climate 
zones that are spread apart geographically and have higher variability in 
rainfall. 

```{r}
#SUMMARIZE RAINFALL BY LOCATION
df_rainfall <- summarise(group_by(df, Location), 
                         Mean_rainfall = mean(Rainfall, na.rm = TRUE),
                         SD_rainfall=sd(Rainfall, na.rm = TRUE))
df_rainfall[with(df_rainfall, order(SD_rainfall, decreasing = TRUE)),]
```
The Australian government has identified 8 climate zones.

We will keep Cairns, Brisbane, Sydney with SydneyAirport, Moree, 
Perth with PerthAirport, and AliceSprings.

* Cairns - Climate Zone 1
* Brisbane - Climate Zone 2
* Alice Springs - Climate Zone 3
* Moree - Climate Zone 4
* Perth - Climate Zone 5
* Sydney - Climate Zone 5/6

```{r}
#CREATE DATAFRAME OF CHOSEN LOCATIONS
loc_keep <- c("Cairns","Brisbane","AliceSprings","Moree","Perth",
              "PerthAirport","Sydney","SydneyAirport")
df_remove3 <- subset(df,!Location %in% c(loc_keep))
df <- subset(df,Location %in% c(loc_keep))
```

## Summary Statistics for Chosen Locations

```{r}
#SUMMARIZE DATA BY COLUMN
summary(df)
```

```{r}
#CHECK NUMBER OF UNIQUE DATES AND TIME SPAN
num_unique_dates <- length(unique(df$Date))
time_difference <- as.numeric((max(df$Date)-min(df$Date)), units="days")
total_years <- time_difference/365
num_unique_locations <- length(unique(df$Location))
```

Using the filtered data set, there are weather observations for 1859 unique 
dates across 8 unique locations over the time span of approximately 5.3 years.

```{r}
#CALCULATE NUMBER OF MISSING DATES
num_missing_dates <- time_difference - num_unique_dates
```

There are 1859 unique dates in the data set, however there is a difference of
1947 days between the first observation and the last observation, which means
there are still 88 dates missing in the time span.

## Detecting Missing Values for Chosen Locations

```{r}
#Display a table that shows the number of rows with a certain number of NA's
mis_ind = rowSums(is.na(df))
table(mis_ind)
```

```{r}
#Plot the distribution of the number of NA's per row
hist(mis_ind)
```
```{r}
#Summarizes the number of NA's per variable.
mis_col = colSums(is.na(df))
mis_col
```


```{r}
#Summarizes the number of NA's per variable as percentage.
mis_col_percent <- round((mis_col/nrow(df)*100), digits = 2)
mis_col_percent
```

## Splitting Data by Locations

```{r}
df_Cairns <- subset(df,Location == "Cairns")
df_Brisbane <- subset(df,Location == "Brisbane")
df_AliceSprings <- subset(df,Location == "AliceSprings")
df_Moree <- subset(df,Location == "Moree")
df_Perth <- subset(df,Location %in% c("Perth","PerthAirport"))
df_Sydney <- subset(df,Location %in% c("Sydney","SydneyAirport"))

```

## Summarizing Data & NA's by Location

### Cairns

```{r}
summary(df_Cairns)
````
````{r}
#CHECK NUMBER OF UNIQUE DATES AND TIME SPAN
num_unique_dates_Cairns <- length(unique(df_Cairns$Date))
time_difference_Cairns <- as.numeric((max(df_Cairns$Date)-min(df_Cairns$Date)), units="days")
total_years_Cairns <- time_difference/365
num_missing_dates_Cairns <- time_difference_Cairns - num_unique_dates_Cairns
num_missing_dates_Cairns
````

### Brisbane

```{r}
summary(df_Brisbane)
````

````{r}
#CHECK NUMBER OF UNIQUE DATES AND TIME SPAN
num_unique_dates_Brisbane <- length(unique(df_Brisbane$Date))
time_difference_Brisbane <- as.numeric((max(df_Brisbane$Date)-min(df_Brisbane$Date)), units="days")
total_years_Brisbane <- time_difference/365
num_missing_dates_Brisbane <- time_difference_Brisbane - num_unique_dates_Brisbane
num_missing_dates_Brisbane
````

### Alice Springs

```{r}
summary(df_AliceSprings)
````

````{r}
#CHECK NUMBER OF UNIQUE DATES AND TIME SPAN
num_unique_dates_Alice <- length(unique(df_AliceSprings$Date))
time_difference_Alice <- as.numeric((max(df_AliceSprings$Date)-min(df_AliceSprings$Date)), units="days")
total_years_Alice <- time_difference/365
num_missing_dates_Alice <- time_difference_Alice - num_unique_dates_Alice
num_missing_dates_Alice
````
