---
title: "Classification Tree - Brisbane"
author: "Kathryn Weissman"
date: "1/4/2022"
output:
  pdf_document: default
  html_document: default
---

```{r clean workspace, echo=FALSE}
# Clean workspace
rm(list=ls())
```

# Classification Tree: Brisbane

The goal is to predict if there will be rain the following day.

```{r setup, include=FALSE}
# these are the libraries included in CART Lab
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(tree)
library(performanceEstimation)
```

```{r}
# The random seed must be set before each call to a function that uses random.
set.seed(1234) # for reproducibility of results
```


## Load Train & Test Data

I am loading the same data that was used for the LDA modelling.

```{r load data}
# Load the data
Btrain <- read.csv("Train_Test_CSVs/df_Brisbane_train.csv", stringsAsFactors = T)
Btest <- read.csv("Train_Test_CSVs/df_Brisbane_test.csv", stringsAsFactors = T)
Btrain$Date <- as.Date(Btrain$Date)
Btest$Date <- as.Date(Btest$Date)
```

## Summarize Train Data
```{r}
str(Btrain)
```
```{r}
summary(Btrain)
```

## Summarize Test Data
```{r}
summary(Btest)
```

## Compare Target Variables for Train and Test Data

It is important that our training data and testing data have similar characteristics in order to optimize the performance of our model. The test set has a slightly lower percentage of rainy days than the training set.

```{r}
print ("Percentage of Days with Rain Tomorrow in Train Data")
round(prop.table(table(Btrain$RainTomorrow))*100,1)
print ("Percentage of Days with Rain Tomorrow in Test Data")
round(prop.table(table(Btest$RainTomorrow))*100,1)
```

The seasons are mostly balanced between the training and testing data. The testing data has a slightly larger proportion of winter days. This is due to the span of dates in the training data not including one of the full years. The training data spans from July 1, 2008 to May 31, 2012. We don't have data for the month of June 2008 to include in the training set.

```{r}
print ("Percentage of Days in each Season in Train Data")
round(prop.table(table(Btrain$Season))*100,1)
print ("Percentage of Days in each Season in Test Data")
round(prop.table(table(Btest$Season))*100,1)
```


## Classification Tree

<https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf>

"The rpart programs build classification or regression models of a very general structure using a two stage procedure; the resulting models can be represented as binary trees."

We use two different sets of modeling variables to see if there is a difference in the performance of the model for classifying whether or not there will be rain tomorrow.

```{r choose modeling variables}
# We use two different sets of variables for the model to consider

# Set 1 includes "RainToday" and "TempRange"
modeling_vars1 <- c("Evaporation", "Sunshine", "WindGustSpeed", "WindSpeed9am", 
                   "WindSpeed3pm", "Humidity9am", "Humidity3pm", "Pressure9am", 
                   "Pressure3pm", "Cloud9am", "Cloud3pm", "TempRange", 
                   "RainToday", "Season", "RainTomorrow")

# Set 2 includes all temperature variables and "Rainfall" in addition to "RainToday"
modeling_vars2 <- c("Evaporation", "Sunshine", "WindGustSpeed", "WindSpeed9am",
                   "WindSpeed3pm", "Humidity9am", "Humidity3pm", "Pressure9am",
                   "Pressure3pm",  "Cloud9am", "Cloud3pm", "Temp9am", "Temp3pm",
                   "TempRange", "MaxTemp", "MinTemp","RainToday", "Rainfall", "Season", 
                   "RainTomorrow")


train1 <- Btrain[,modeling_vars1]
test1 <- Btest[,modeling_vars1]

train2 <- Btrain[,modeling_vars2]
test2 <- Btest[,modeling_vars2]
```

### SMOTE algorithm for unbalanced classification problems

From the library {performanceEstimation}

"This function handles unbalanced classification problems using the SMOTE method. Namely, it can generate a new "SMOTEd" data set that addresses the class unbalance problem."

Balanced Training Sets 1 and 2 have different observations due to the nearest neighbors defined by the subset of variables contained in each training data set.

```{r}
set.seed(1234) # for reproducibility of results
# Create balanced training data sets
trainBal1 <- smote(RainTomorrow ~., train1, perc.over = 2, k = 5, perc.under = 2)
trainBal2 <- smote(RainTomorrow ~., train2, perc.over = 2, k = 5, perc.under = 2)
```

```{r}
print("Training Data: Count of Rain Tomorrow")
(table(Btrain$RainTomorrow))
print("Balanced Training 1 Data: Count of Rain Tomorrow")
(table(trainBal1$RainTomorrow))
print("Balanced Training 1 Data: Percent of Days with Rain Tomorrow")
round(prop.table((table(trainBal1$RainTomorrow)))*100,2)
print("Balanced Training 2 Data: Count of Rain Tomorrow")
(table(trainBal2$RainTomorrow))
print("Balanced Training 2 Data: Percent of Days with Rain Tomorrow")
round(prop.table(table(trainBal2$RainTomorrow))*100,2)
```


```{r}
print("Balanced Training 1 Data: Percent of Days in each Season")
round(prop.table(table(trainBal1$Season))*100,1)
print("Balanced Training 2 Data: Percent of Days in each Season")
round(prop.table(table(trainBal2$Season))*100,1)
```

### Using Fitting & Pruning Strategy shown in Lab

#### First Set of Variables on Unbalanced Data

```{r}
# Best strategy for tree fitting, start with cp = 0, then prune.
set.seed(1234) # for reproducibility of results
treeFit1 <- rpart(RainTomorrow ~., data = train1, method = "class", cp = 0) 
printcp(treeFit1)
plotcp(treeFit1)
#rpart.plot(treeFit1)
```

```{r}
# Find the cp with lowest error, then prune.
xerror <- treeFit1$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeFit1$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeFit1$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeFit1$cptable[icp, "CP"]
```

The pruned tree produced using imbalanced training data on the first set of variables is extremely simple, and only uses two variables, Humidity3pm and WindGustSpeed.

```{r}
# prune using cp
tree1 <- prune(treeFit1, cp = cp)
rpart.plot(tree1)
```

```{r, include=FALSE}
#Classification Rules
rpart.rules(tree1, style = "tall")
```

```{r}
#Checking important variables
importance1 <- tree1$variable.importance 
importance1 <- round(100*importance1/sum(importance1), 1)
importance1[importance1 >= 1]
```

##### Confusion Matrix

Help for Confusion Matrix: <https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62>

Recall,Precision and Accuracy should be high as possible

Balanced Accuracy represents area under ROC.

Although the accuracy is fairly high, 84%, the sensitivity is low, below 60%, which is how well the model predicts it will rain on a rainy day. Since the data is imbalanced, we should try using SMOTE sampling for the training data to see if it improves the performance of the model.

```{r}
#Evaluation
#Confusion matrix-train
pred_train1 <- predict(tree1, train1, type = 'class') # using train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_train1, train1$RainTomorrow, positive="Yes")
```

#### First Set of Variables on Balnced Training Data using SMOTE

```{r}
# Best strategy for tree fitting, start with cp = 0, then prune.
set.seed(1234) # for reproducibility of results
treeFitBal1 <- rpart(RainTomorrow ~., data = trainBal1, method = "class", cp = 0) 
printcp(treeFitBal1)
plotcp(treeFitBal1)
#rpart.plot(treeFitBal1)
```

```{r}
# Find the cp with lowest error, then prune.
xerror <- treeFitBal1$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeFitBal1$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeFitBal1$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeFitBal1$cptable[icp, "CP"]
```

The pruned tree using the balanced data is much more complex than the tree produced using imbalanced data.

```{r}
# prune using cp
treeBal1 <- prune(treeFitBal1, cp = cp)
rpart.plot(treeBal1)
```

```{r}
#Classification Rules
rpart.rules(treeBal1, style = "tall")
```
In the Imbalanced Training Data for the first set of variables, Humidity3pm, Humidity9am, Sunshine, TempRange, Cloud3pm, Cloud9am, WindSpeed3pm, and WindGustSpeed  were the only variables with important greater than 1%. Using the balanced training data spreads out the importance to more variables including WindSpeed9am, Pressure variables, Evaporation, and Season.

```{r}
#Checking important variables
importanceBal1 <- treeBal1$variable.importance 
importanceBal1 <- round(100*importanceBal1/sum(importanceBal1), 1)
importanceBal1[importanceBal1 >= 1]
```

Using the model created by balancing the data produces better results when checking predictions on the training data. Accuracy improved from 84.5% to 86.2%. Sensitivity improved from 54.5% to 78.2%. Specificity decreased from 94.8% to 89%, but Balanced Accuracy (Area under ROC) improved from 74.6% to 83.6%

```{r}
#Evaluation of model created with balanced data
#Confusion matrix-train
pred_trainBal1 <- predict(treeBal1, train1, type = 'class') # using original train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_trainBal1, train1$RainTomorrow, positive="Yes")
```

The default probability threshold is 50% for classification. We can use trial and error to determine if a different probability threshold improves results.

In this case, lowering the probability threshold to 30% did not improve our model evaluation metrics.

When the tree produces a probability of greater than 30% chance of rain tomorrow, predicting that it will rain lowers the balanced accuracy of the model.

We can use the default threshold of 50%.

```{r}
#Train Set Evaluation of Balanced Model with probabilities
#Confusion matrix-train
pred_trainBal1_prob <- predict(treeBal1, train1, type = 'prob') # using imbalanced training data
# predict rain if chance of rain is more than 30% (default is 50%)
pred_trainBal1_prob30 <- ifelse(pred_trainBal1_prob[,2]>0.3,"Yes","No") 
confusionMatrix(data= as.factor(pred_trainBal1_prob30), train1$RainTomorrow, positive="Yes")
```

Some of our key metrics decrease slightly when expanded to the test set, which could be an indicator of overfitting to the training data, but it is not too different.

Accuracy is 80%, Sensitivity is 79.7%, Specificity is 80%, and Balanced Accuracy is 80%.

```{r}
#Test Set Evaluation of Balanced Model 1
#Confusion matrix-test
pred_testBal1 <- predict(treeBal1, test1, type = 'class') # using testing data
confusionMatrix(pred_testBal1, test1$RainTomorrow, positive="Yes")
```

#### Second Set of Variables

This set includes more variables than the first set.
Set 1 included "RainToday", but set 2 also includes "Rainfall". 
Set 1 included "TempRange", but set 2 includes all temperature related varaiables including TempRange.

```{r}
# Best strategy for tree fitting, cp = 0
set.seed(1234) # for reproducibility of results
treeFit2 <- rpart(RainTomorrow ~., data = train2, method = "class", cp = 0) 
printcp(treeFit2)
plotcp(treeFit2)
#rpart.plot(treeFit2)
```

```{r}
xerror <- treeFit2$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeFit2$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeFit2$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeFit2$cptable[icp, "CP"]
```

After pruning, the trees for both sets of variables that use the imbalanced training data are the identical for Brisbane, and they only use the Humidity3pm and WindGustSpeed variable to make a prediction.

```{r}
tree2 <- prune(treeFit2, cp = cp)
rpart.plot(tree2)
```
```{r, include=FALSE}
#Classification Rules
rpart.rules(tree2, style = "tall")
```

The important variables are the same for tree1 and tree2, even though some different variables were added to the second training set. Humidity3pm is the most important.

```{r, include=FALSE}
#Checking important variables
importance2 <- tree2$variable.importance 
importance2 <- round(100*importance2/sum(importance2), 1)
importance2[importance2 >= 1]
```

#### Second Set of Variables on Balnced Training Data using SMOTE

The next step is to create the tree using a more balanced training set with the second set of variables to see if our model performance improves.

```{r}
# Best strategy for tree fitting, cp = 0
set.seed(1234) # for reproducibility of results
treeBalFit2 <- rpart(RainTomorrow ~., data = trainBal2, method = "class", cp = 0) 
printcp(treeBalFit2)
plotcp(treeBalFit2)
#rpart.plot(treeBalFit2)
```

```{r}
xerror <- treeBalFit2$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeBalFit2$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeBalFit2$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeBalFit2$cptable[icp, "CP"]
```

```{r}
treeBal2 <- prune(treeBalFit2, cp = cp)
rpart.plot(treeBal2)
```

```{r}
#Classification Rules
rpart.rules(treeBal2, style = "tall")
```
The tree using balanced training data with the second set of variables identified more variables with importance greater than 1% than the tree using the first set of variables, and they are also in a different order, however Humidity3pm is still the most important variable. All of the temperature variables have importance greater than 1% and Rainfall has importance of 1%.

```{r}
#Checking important variables
importanceBal2 <- treeBal2$variable.importance 
importanceBal2 <- round(100*importanceBal2/sum(importanceBal2), 1)
importanceBal2[importanceBal2 >= 1]
```

The model using the second set of variables performs slightly better than the model created with the first set when evaluating the predictions on the same set of training data. This indicates that the rpart algorithm did a good job of choosing the important variables to use in the model.

Accuracy improves from 86.2% to 89.6%. Sensitivity improves from 78.2% to 79.8%. Specificity improves from 89% to 93%. Balanced accuracy improves from 83.6% to 86.4%

```{r}
#Evaluation of second model using Training Set
#Confusion matrix-train
pred_trainBal2 <- predict(treeBal2, train2, type = 'class') # using unbalanced train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_trainBal2, train2$RainTomorrow, positive="Yes")
```

Again we will leave the default of 50% probability to make the prediction, because when lowering the probability to 30%, the model performs slightly worse.

```{r}
#Train Set Evaluation with probabilities
#Confusion matrix-train
pred_trainBal2_prob <- predict(treeBal2, train2, type = 'prob') # using train data
# predict rain if chance of rain is more than 30% (default is 50%)
pred_trainBal2_prob30 <- ifelse(pred_trainBal2_prob[,2]>0.3,"Yes","No") 
confusionMatrix(data= as.factor(pred_trainBal2_prob30), train2$RainTomorrow, positive="Yes")
```

Both balanced models with the two different sets of variables performed similarly when evaluated with the test data set. Accuracy decreased from 80% to 79% and Balanced Accuracy decreased from 80% to 75%. The sensitivity of the first model was better, with 80% compared to the second model's 69%, but the specificity was a little higher with the second model at 82% compare to 80%.

```{r}
#Test Set Evaluation of Balanced Model 2
#Confusion matrix-test
pred_testBal2 <- predict(treeBal2, test2, type = 'class') # using testing data
confusionMatrix(pred_testBal2, test2$RainTomorrow, positive="Yes")
```

Balanced Model 1 performs better with the test data and should be the model that is implemented.