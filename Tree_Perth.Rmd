---
title: "Classification Tree - Perth"
author: "Kathryn Weissman"
date: "1/4/2022"
output:
  pdf_document: default
  html_document: default
---

```{r clean workspace, echo=FALSE}
# Clean workspace
rm(list=ls())
```

# Classification Tree: Perth

The goal is to predict if there will be rain the following day.

```{r setup, include=FALSE}
# these are the libraries included in CART Lab
library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(tree)
library(performanceEstimation)
```

```{r}
set.seed(1234) # for reproducibility of results
```


## Load Train & Test Data

I am loading the same data that was used for the LDA modelling.

```{r load data}
# Load the data
Ptrain <- read.csv("Train_Test_CSVs/df_Perth_train.csv", stringsAsFactors = T)
Ptest <- read.csv("Train_Test_CSVs/df_Perth_test.csv", stringsAsFactors = T)
Ptrain$Date <- as.Date(Ptrain$Date)
Ptest$Date <- as.Date(Ptest$Date)
```

## Summarize Train Data
```{r}
str(Ptrain)
```
```{r}
summary(Ptrain)
```

## Summarize Test Data
```{r}
summary(Ptest)
```

## Compare Target Variables for Train and Test Data

It is important that our training data and testing data have similar characteristics to check the accuracy of our model.

```{r}
print ("Percentage of Days with Rain Tomorrow in Train Data")
round(prop.table(table(Ptrain$RainTomorrow))*100,1)
print ("Percentage of Days with Rain Tomorrow in Test Data")
round(prop.table(table(Ptest$RainTomorrow))*100,1)
```

The seasons are mostly balanced between the training and testing data. The testing data has a slightly larger proportion of winter days. This is due to the span of dates in the training data not including one of the full years. The training data spans from July 1, 2008 to May 31, 2012. We don't have data for the month of June 2008 to include in the training set.

```{r}
print ("Percentage of Days in each Season in Train Data")
round(prop.table(table(Ptrain$Season))*100,1)
print ("Percentage of Days in each Season in Test Data")
round(prop.table(table(Ptest$Season))*100,1)
```


## Classification Tree

<https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf>

"The rpart programs build classification or regression models of a very general structure using a two stage procedure; the resulting models can be represented as binary trees."

We use two different sets of modeling variables to see if there is a difference in the performance of the model for classifying whether or not there will be rain tomorrow.

```{r choose modeling variables}
# We use two different sets of variables for the model to consider

# Set 1 includes "RainToday" and "TempRange"
modeling_vars1 <- c("Evaporation", "Sunshine", "WindGustSpeed", "WindSpeed9am", 
                   "WindSpeed3pm", "Humidity9am", "Humidity3pm", "Pressure9am", 
                   "Pressure3pm", "Cloud9am", "Cloud3pm", "TempRange", 
                   "RainToday", "Season", "RainTomorrow")

# Set 2 includes all temperature variables and "Rainfall" instead of "RainToday"
modeling_vars2 <- c("Evaporation", "Sunshine", "WindGustSpeed", "WindSpeed9am",
                   "WindSpeed3pm", "Humidity9am", "Humidity3pm", "Pressure9am",
                   "Pressure3pm",  "Cloud9am", "Cloud3pm", "Temp9am", "Temp3pm",
                   "TempRange", "MaxTemp", "MinTemp", "Rainfall", "Season", 
                   "RainTomorrow")

train1 <- Ptrain[,modeling_vars1]
test1 <- Ptest[,modeling_vars1]

train2 <- Ptrain[,modeling_vars2]
test2 <- Ptest[,modeling_vars2]
```


### SMOTE algorithm for unbalanced classification problems

From the library {performanceEstimation}

"This function handles unbalanced classification problems using the SMOTE method. Namely, it can generate a new "SMOTEd" data set that addresses the class unbalance problem."

Balanced Training Sets 1 and 2 have different observations due to the nearest neighbors defined by the subset of variables contained in each training data set.

```{r}
set.seed(1234) # for reproducibility of results
# Create balanced training data sets
trainBal1 <- smote(RainTomorrow ~., train1, perc.over = 2, k = 5, perc.under = 2)
trainBal2 <- smote(RainTomorrow ~., train2, perc.over = 2, k = 5, perc.under = 2)
```


```{r}
print("Training Data: Count of Rain Tomorrow")
(table(Ptrain$RainTomorrow))
print("Balanced Training 1 Data: Count of Rain Tomorrow")
(table(trainBal1$RainTomorrow))
print("Balanced Training 1 Data: Percent of Days with Rain Tomorrow")
round(prop.table((table(trainBal1$RainTomorrow)))*100,2)
print("Balanced Training 2 Data: Count of Rain Tomorrow")
(table(trainBal2$RainTomorrow))
print("Balanced Training 2 Data: Percent of Days with Rain Tomorrow")
round(prop.table(table(trainBal2$RainTomorrow))*100,2)
```

```{r}
print("Balanced Training 1 Data: Percent of Days in each Season")
round(prop.table(table(trainBal1$Season))*100,1)
print("Balanced Training 2 Data: Percent of Days in each Season")
round(prop.table(table(trainBal2$Season))*100,1)
```

### Using Fitting & Pruning Strategy shown in Lab

#### First Set of Variables using Imbalanced Training Data


```{r}
# Best strategy for tree fitting, cp = 0
set.seed(1234) # for reproducibility of results
treeFit1 <- rpart(RainTomorrow ~., data = train1, method = "class", cp = 0) 
printcp(treeFit1)
plotcp(treeFit1)
rpart.plot(treeFit1)
```
```{r}
xerror <- treeFit1$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeFit1$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeFit1$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeFit1$cptable[icp, "CP"]
```

The pruned tree using imbalanced data is easy to understand, and uses five variables to make the splits.

```{r}
tree1 <- prune(treeFit1, cp = cp)
rpart.plot(tree1)
```

```{r}
#Classification Rules
rpart.rules(tree1, style = "tall")
```

```{r}
#Checking important variables
importance1 <- tree1$variable.importance 
importance1 <- round(100*importance1/sum(importance1), 1)
importance1[importance1 >= 1]
```

### Confusion Matrix

Help for Confusion Matrix: <https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62>

Recall,Precision and Accuracy should be high as possible

Balanced Accuracy represents area under ROC.

Although the accuracy is high, 90%, the sensitivity is lower, at 70%, which is how well the model predicts it will rain on a rainy day. Since the data is imbalanced, we should try using SMOTE sampling for the training data to see if it improves the performance of the model.

```{r}
#Evaluation
#Confusion matrix-train
pred_train1 <- predict(tree1, train1, type = 'class') # using train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_train1, train1$RainTomorrow, positive="Yes")
```

The sensitivity is very low, which is how accurate the predictions are for rainy days. Since the data is imbalanced, we should try using SMOTE sampling for the training data to see if it improves the performance of the model.

#### First Set of Variables on Balnced Training Data using SMOTE

This is the model that performs the best when evaluating it on the test set.

```{r}
# Best strategy for tree fitting, start with cp = 0, then prune.
set.seed(1234) # for reproducibility of results
treeFitBal1 <- rpart(RainTomorrow ~., data = trainBal1, method = "class", cp = 0) 
printcp(treeFitBal1)
plotcp(treeFitBal1)
#rpart.plot(treeFitBal1)
```
```{r}
# Find the cp with lowest error, then prune.
xerror <- treeFitBal1$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeFitBal1$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeFitBal1$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeFitBal1$cptable[icp, "CP"]
```

```{r}
# prune using cp
treeBal1 <- prune(treeFitBal1, cp = cp)
rpart.plot(treeBal1)
```

```{r}
#Classification Rules
rpart.rules(treeBal1, style = "tall")
```
In the Imbalanced Training Data for the first set of variables, Humidity3pm, Sunshine, WindGustSpeed, Evaporation, and TempRange were the 5 most important variables. For the balanced training data, Cloud3pm, and Cloud9am are more important than Evaporation and WindGustSpeed.

```{r}
#Checking important variables
importanceBal1 <- treeBal1$variable.importance 
importanceBal1 <- round(100*importanceBal1/sum(importanceBal1), 1)
importanceBal1[importanceBal1 >= 1]
```

Using the model created by balancing the data produces better results when checking predictions on the training data. Accuracy decreased from 90.8% to 88%, however Sensitivity improved from 70.2% to 87.2%. Specificity decreased from 95.5% to 88.3%, but Balanced Accuracy (Area under ROC) improved from 82.8% to 87.7%

```{r}
#Evaluation of model created with balanced data
#Confusion matrix-train
pred_trainBal1 <- predict(treeBal1, train1, type = 'class') # using original train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_trainBal1, train1$RainTomorrow, positive="Yes")
```

Some of our key metrics decrease slightly when expanded to the test set, which could be an indicator of overfitting to the training data, but it is not too different.

Accuracy decreased from 88% to 83.6%, Sensitivity decreased from 87.2% to 75.3%, Specificity decreased from 88 to 85.8%, and Balanced Accuracy decreased from 87.7 to 80.6%.

```{r}
#Test Set Evaluation of Balanced Model 1
#Confusion matrix-test
pred_testBal1 <- predict(treeBal1, test1, type = 'class') # using testing data
confusionMatrix(pred_testBal1, test1$RainTomorrow, positive="Yes")
```

#### Second Set of Variables

#### Imbalanced Data

This set includes more variables than the first set.
Set 1 included "RainToday", but set 2 includes "Rainfall". 
Set 1 included "TempRange", but set 2 includes all temperature related variables including TempRange.

```{r}
# Best strategy for tree fitting, cp = 0
set.seed(1234) # for reproducibility of results
treeFit2 <- rpart(RainTomorrow ~., data = train2, method = "class", cp = 0) 
printcp(treeFit2)
plotcp(treeFit2)
rpart.plot(treeFit2)
```
```{r}
xerror <- treeFit2$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeFit2$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeFit2$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeFit2$cptable[icp, "CP"]
```

After pruning, the tree for the second set of variables is extremely simple, using just 3 variables: Humidity3pm, Sunshine, and WindGustSpeed.

```{r}
tree2 <- prune(treeFit2, cp = cp)
rpart.plot(tree2)
```
```{r}
#Classification Rules
rpart.rules(tree2, style = "tall")
```

For imbalanced training data, 4 of the 5 most important variables are the same in the second set of variables. The difference is that Temp3pm is considered more important than Evaporation in the second set.

```{r}
#Checking important variables
importance2 <- tree2$variable.importance 
importance2 <- round(100*importance2/sum(importance2), 1)
importance2[importance2 >= 1]
```

Training the model with the second set of imbalanced training data had worse results than the first set of variables. Specificity was the only metric that was better, increasing from 95.5% to 87%. Sensitivity decreased from 70% to 56% and Balanced Accuracy decreased from 82.8% to 76.6%.

Next, we will check if the SMOTE'd data set performs better with the second set of variables than the first set.

```{r}
#Train Set Evaluation
#Confusion matrix-train
pred_train2 <- predict(tree2, train2, type = 'class') # using train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_train2, train2$RainTomorrow, positive="Yes")
```


#### Second Set of Variables on Balnced Training Data using SMOTE

```{r}
# Best strategy for tree fitting, cp = 0
set.seed(1234) # for reproducibility of results
treeBalFit2 <- rpart(RainTomorrow ~., data = trainBal2, method = "class", cp = 0) 
printcp(treeBalFit2)
plotcp(treeBalFit2)
#rpart.plot(treeBalFit2)
```
```{r}
xerror <- treeBalFit2$cptable[,"xerror"]
imin.xerror <- which.min(xerror)
treeBalFit2$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + treeBalFit2$cptable[imin.xerror, "xstd"]
icp <- min(which(xerror <= upper.xerror))
cp <- treeBalFit2$cptable[icp, "CP"]
```

```{r}
treeBal2 <- prune(treeBalFit2, cp = cp)
rpart.plot(treeBal2)
```

```{r}
#Classification Rules
rpart.rules(treeBal2, style = "tall")
```
The tree trained with the balanced second set of variables gave Rainfall the second most importance of all the variables, which is a big difference because RainToday was not an important variable in the first set of variables. Sunshine, Cloud3pm, Humidity3pm, and TempRange are common important variables between the two sets.

```{r}
#Checking important variables
importanceBal2 <- treeBal2$variable.importance 
importanceBal2 <- round(100*importanceBal2/sum(importanceBal2), 1)
importanceBal2[importanceBal2 >= 1]
```

The model using the balanced second set of variables performs similar to the model created with the first set when evaluating the predictions on the same set of training data. 

Accuracy improves from 88% to 88.7%. Sensitivity decreases from 87.2% to 84%. Specificity improves from 88.3% to 89.7%. Balanced accuracy decreased from 87.7% to 86.9%

```{r}
#Evaluation of second model using Training Set
#Confusion matrix-train
pred_trainBal2 <- predict(treeBal2, train2, type = 'class') # using unbalanced train data
#Make sure to state positive class in the confusion matrix.
confusionMatrix(pred_trainBal2, train2$RainTomorrow, positive="Yes")
```

The second set of variables appears to be overfitting the model, because our metrics are worse using the second set. Accuracy decreases from 83.6% to 80.3%, Sensitivity decreases from 75% to 59.7%, Specificity remained the same at 85.8%, and Balanced Accuracy decreased from 80.6% to 72.8%.

```{r}
#Test Set Evaluation of Balanced Model 2
#Confusion matrix-test
pred_testBal2 <- predict(treeBal2, test2, type = 'class') # using test data
confusionMatrix(pred_testBal2, test2$RainTomorrow, positive="Yes")
```

Balanced Model 1 performs better with the test data and should be the model that is implemented.
